{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm as tqdm\n",
    "import sys\n",
    "import yaml\n",
    "import datetime\n",
    "\n",
    "from typing import Union, Optional\n",
    "from datasets import load_dataset, Dataset\n",
    "from dotenv import load_dotenv\n",
    "from omegaconf import OmegaConf\n",
    "from itertools import cycle\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import get_peft_model, LoraConfig\n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "project_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.insert(0, project_dir)\n",
    "\n",
    "from latent_at.laa.attacks import *\n",
    "from latent_at.lat_datasets import process_pretokenized_dataset, WMDPLATTargetedDataCollator, WMDPLATDataCollator\n",
    "from latent_at.laa import add_hooks, clear_hooks\n",
    "from latent_at.lat_methods import WMDPProjectedGradLAT\n",
    "from latent_at.wmdp.rmu.utils import get_data\n",
    "from tasks.general_capabilities.multiple_choice_tasks import WMDPTask, MMLUTask\n",
    "from tasks.wmdp.WMDP_MCTask import WMDP_MCTask\n",
    "from wmdp_utils import evaluate_during_training, eval_and_log, load_model\n",
    "\n",
    "load_dotenv()\n",
    "hf_access_token = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config file as OmegaConf object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(file_path: str) -> dict:\n",
    "    with open(file_path, 'r') as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "    return config\n",
    "\n",
    "config: dict = load_config(\"wmdp_ga_lat_config.yaml\")\n",
    "config = OmegaConf.create(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model\n",
    "Make sure you have a .env file in the same directory as this notebook with a HuggingFace read token with access to Llama 7B chat. Set this to the variable `HUGGINGFACE_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {\n",
    "    \"LLAMA_7B\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"LLAMA_34B\": \"meta-llama/Llama-2-34b-chat-hf\",\n",
    "    \"ZEPHYR_7B\": \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    \"ZEPHYR_34B\": \"HuggingFaceH4/zephyr-34b-beta\",\n",
    "}\n",
    "\n",
    "\n",
    "model, tokenizer = load_model(hf_access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data if first time loading to system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"bio-retain-corpus\", \"cyber-forget-corpus\", \"cyber-retain-corpus\"] # Everything on cais/wmdp-corpora\n",
    "for name in dataset_names:\n",
    "    if not os.path.exists(f\"wmdp/data/{name}.jsonl\"):\n",
    "        dataset = load_dataset('cais/wmdp-corpora', name)\n",
    "        dataset[\"train\"].to_json(f'wmdp_data/{name}.jsonl', batch_size=128)\n",
    "\n",
    "\"\"\"TODO - add WMDP bio forget loading code here\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_targeted_wmdp_data(config: OmegaConf) -> tuple[Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Forget and retain datasets differ. Load forget and retain then recombine into columns 'adv_tokens' and 'def_tokens' in new Huggingface Dataset object.\n",
    "\n",
    "    Supports bio/cyber WMDP retain corpora and WikiText (paired with both bio and cyber unlearn) as options for retain dataset. See config line 'retain_corpora'.\n",
    "    \"\"\"\n",
    "    retain_corpora = config.retain_corpora.split(\",\")\n",
    "    forget_corpora = config.forget_corpora.split(\",\")\n",
    "\n",
    "    # Load and rename datasets for 'forget' corpora\n",
    "    # Only bio forget needs to be locally loaded\n",
    "    hf_location = \"cais/wmdp-corpora\"\n",
    "    for d in forget_corpora:\n",
    "        dataset_path = f\"wmdp_data/{d}.jsonl\"\n",
    "        if d == \"bio-forget-corpus\":\n",
    "            bio_forget = load_dataset('json', data_files=dataset_path, split='train')\n",
    "            bio_forget = bio_forget.rename_column('text', 'adv_tokens')\n",
    "        elif d == \"cyber-forget-corpus\":\n",
    "            cyber_forget = load_dataset(hf_location, name=d, split='train')\n",
    "            cyber_forget = cyber_forget.rename_column('text', 'adv_tokens')\n",
    "\n",
    "    if retain_corpora == ['bio-retain-corpus', 'cyber-retain-corpus']:\n",
    "        for d in retain_corpora:\n",
    "            retain_dataset = load_dataset(hf_location, name=d, split='train')\n",
    "            retain_dataset = retain_dataset.rename_column('text', 'def_tokens')\n",
    "            if d == \"bio-retain-corpus\":\n",
    "                bio_retain = retain_dataset\n",
    "            elif d == \"cyber-retain-corpus\":\n",
    "                cyber_retain = retain_dataset\n",
    "    elif retain_corpora == ['wikitext', 'wikitext']:\n",
    "        bio_retain = cyber_retain = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "        bio_retain = bio_retain.rename_column('text', 'def_tokens')\n",
    "        cyber_retain = cyber_retain.rename_column('text', 'def_tokens')\n",
    "\n",
    "    def merge_rows(example1, example2):\n",
    "        return {'adv_tokens': example1['adv_tokens'], 'def_tokens': example2['def_tokens']}\n",
    "\n",
    "    min_bio_length, min_cyber_length = min(len(bio_forget), len(bio_retain)), min(len(cyber_forget), len(cyber_retain))\n",
    "    bio_dataset = bio_forget.select(range(min_bio_length)).map(\n",
    "        lambda x, \n",
    "        idx: merge_rows(x, bio_retain[idx]),\n",
    "        with_indices=True,\n",
    "    ).remove_columns(['title', 'abstract', 'doi'])\n",
    "    cyber_dataset = cyber_forget.select(range(min_cyber_length)).map(\n",
    "        lambda x, \n",
    "        idx: merge_rows(x, cyber_retain[idx]), \n",
    "        with_indices=True\n",
    "    )\n",
    "\n",
    "    return bio_dataset, cyber_dataset\n",
    "\n",
    "\n",
    "def make_targeted_wmdp_dataloaders(\n",
    "    config: OmegaConf,\n",
    "    bio_dataset: Dataset,\n",
    "    cyber_dataset: Dataset, \n",
    "    tokenizer: AutoTokenizer, \n",
    ") -> tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Don't use for anything except WMDP exactly as loaded with tuple list [bio, cyber].\n",
    "\n",
    "    Returns:\n",
    "        Two dataloaders: one for bio and one for cyber.\n",
    "    \"\"\"\n",
    "    wmdp_bio_dataloader = DataLoader(\n",
    "        bio_dataset, \n",
    "        shuffle=True, \n",
    "        batch_size=config.lat_batch_size, \n",
    "        collate_fn=WMDPLATTargetedDataCollator(tokenizer, truncate_length=config.data_truncate_length), \n",
    "        drop_last=True,\n",
    "    )\n",
    "    wmdp_cyber_dataloader = DataLoader(\n",
    "        cyber_dataset, \n",
    "        shuffle=True, \n",
    "        batch_size=config.lat_batch_size, \n",
    "        collate_fn=WMDPLATTargetedDataCollator(tokenizer, truncate_length=config.data_truncate_length), \n",
    "        drop_last=True,\n",
    "    )\n",
    "    return wmdp_bio_dataloader, wmdp_cyber_dataloader\n",
    "\n",
    "\n",
    "def make_untargeted_wmdp_dataloaders(\n",
    "    config: OmegaConf,\n",
    "    data: Union[list[str], list[list[str]]], \n",
    "    tokenizer: AutoTokenizer, \n",
    "    sft: Optional[bool] = False,\n",
    ") -> Union[DataLoader, tuple[DataLoader, DataLoader]]:\n",
    "    \"\"\"\n",
    "    Don't use for anything except WMDP unlearn corpora exactly as loaded with tuple list [bio, cyber].\n",
    "    Used for two things: creating SFT dataloaders, and creating WMDP dataloaders in the case where defence and attacker both train on the WMDP unlearn corpora.\n",
    "\n",
    "    In the paper, by default, we use targeted, where the defence trains toward WikiText and the attack trains toward WMDP unlearn corpora. The away losses are gradient ascent on these same datasets but swapped between attack and defence.\n",
    "\n",
    "    Args:\n",
    "        config: OmegaConf object created from yaml file.\n",
    "        data: Each list this contains will be one of the bio/cyber datasets.\n",
    "        sft: If True, data list is two copies of the same dataset. This will only be used to generate supervised fine tuning dataloader for SFT portion of R2D2 loss in LAT.\n",
    "    Returns:\n",
    "        Dataloaders.\n",
    "    \"\"\"\n",
    "    if sft:\n",
    "        return DataLoader(\n",
    "            data, \n",
    "            shuffle=True, \n",
    "            batch_size=config.sft_batch_size, \n",
    "            collate_fn=WMDPLATDataCollator(tokenizer, truncate_length=config.data_truncate_length), \n",
    "            drop_last=True,\n",
    "        )\n",
    "        \n",
    "    wmdp_bio_dataloader = DataLoader(\n",
    "        data[0], \n",
    "        shuffle=True, \n",
    "        batch_size=config.lat_batch_size, \n",
    "        collate_fn=WMDPLATDataCollator(tokenizer, truncate_length=config.data_truncate_length), \n",
    "        drop_last=True,\n",
    "    )\n",
    "    wmdp_cyber_dataloader = DataLoader(\n",
    "        data[1], \n",
    "        shuffle=True, \n",
    "        batch_size=config.lat_batch_size, \n",
    "        collate_fn=WMDPLATDataCollator(tokenizer, truncate_length=config.data_truncate_length), \n",
    "        drop_last=True,\n",
    "    )\n",
    "    return wmdp_bio_dataloader, wmdp_cyber_dataloader\n",
    "\n",
    "\n",
    "def load_sft_dataset(config) -> list[str]:\n",
    "    \"\"\"Works for wikitext and alpaca.\"\"\"\n",
    "    if config.sft_corpora == \"wikitext\":\n",
    "        raw_data = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "    elif config.sft_corpora == \"alpaca\":\n",
    "        raw_data = load_dataset(\"tatsu-lab/alpaca\", \"default\", split=\"train\")\n",
    "    data = []\n",
    "    for x in raw_data:\n",
    "        data.append(str(x['text']))\n",
    "    return data\n",
    "\n",
    "\n",
    "assert config.forget_corpora == 'bio-forget-corpus,cyber-forget-corpus'\n",
    "bio_dataset, cyber_dataset = load_targeted_wmdp_data(config)\n",
    "bio_dataloader, cyber_dataloader = make_targeted_wmdp_dataloaders(config, bio_dataset, cyber_dataset, tokenizer)\n",
    "\n",
    "if config.def_config.def_loss_coefs.sft > 0:\n",
    "    sft_dataset: list[str] = load_sft_dataset(config)\n",
    "    sft_dataloader = make_untargeted_wmdp_dataloaders(config, sft_dataset, tokenizer, sft=True)\n",
    "    sft_dataloader = cycle(sft_dataloader)\n",
    "else:\n",
    "    assert config.def_config.def_loss_coefs[\"sft\"] == 0\n",
    "    sft_dataloader = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lat = WMDPProjectedGradLAT(\n",
    "    model,\n",
    "    dataloader=bio_dataloader if config.use_just_bio else cyber_dataloader,\n",
    "    pgd_layers=config.adv_optim.pgd_layers,\n",
    "    model_layers=config.model_layers,\n",
    "    epsilon=config.adv_optim.epsilon,\n",
    "    init_callback=eval_and_log,\n",
    "    post_adv_callback=eval_and_log,\n",
    "    post_def_callback=eval_and_log,\n",
    "    outer_learning_rate=config.def_config.outer_learning_rate,\n",
    "    inner_learning_rate=config.pgd_config.inner_learning_rate,\n",
    "    num_steps=config.num_epochs,\n",
    "    pgd_iterations_per_step=config.pgd_config.pgd_iterations_per_step,\n",
    "    model_iterations_per_step=config.def_config.model_iterations_per_step,\n",
    "    model_layers_module=config.adv_optim.model_layers_module,\n",
    "    only_train_lora=config.only_train_lora,\n",
    "    sft_dataloader=sft_dataloader,\n",
    "    adv_loss_coefs=config.pgd_config.adv_loss_coefs,\n",
    "    def_loss_coefs=config.def_config.def_loss_coefs,\n",
    "    max_batch_per_acc=config.minibatch_size,\n",
    "    clip_grad=config.clip_grad,\n",
    "    reinitialize_dev_optim=config.def_config.reinitialize_def_optim,\n",
    "    time_limit=config.time_limit,\n",
    "    device=\"cuda\",\n",
    "    pca_kwargs = config.pgd_config.pca_kwargs,\n",
    "    config=config, # Use for more flexible parameter passing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set run name and train. In practice, the hyperparameters set for default WandB run name are those that are varied most often, but these should be changed as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"{config.model_name}_layers-{config.pgd_config.pgd_layers}_epsilon-{config.pgd_config.epsilon}_defsft-{config.def_config.def_loss_coefs.sft:.2f}_deftowards-{config.def_config.def_loss_coefs.toward:.2f}_defaway-{config.def_config.def_loss_coefs.away:.2f}_pgdtowards-{config.pgd_config.adv_loss_coefs.toward:.2f}_defits-{config.def_config.model_iterations_per_step}_pgdits-{config.pgd_config.pgd_iterations_per_step}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "run_lat.train(\"wmdp_lat\", name=run_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
