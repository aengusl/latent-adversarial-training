{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from typing import Optional, Union, Optional\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "import yaml\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import datetime\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "import tqdm as tqdm\n",
    "import wandb\n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "project_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.insert(0, project_dir)\n",
    "\n",
    "from wmdp_utils import evaluate_during_training, eval_and_log, log, load_model\n",
    "from latent_at.wmdp.rmu.utils import get_params, forward_with_cache, get_data\n",
    "from latent_at.laa import clear_hooks\n",
    "from latent_at.lat_methods import projected_gradient_descent\n",
    "\n",
    "load_dotenv()\n",
    "hf_api_key = os.getenv(\"HF_KEY\")\n",
    "hf_write_key = os.getenv(\"HF_WRITE_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To re-download all datasets:\n",
    "Run this code block below if you need to re-download all datasets (location: WMDP folder, data subfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = [\"bio-retain-corpus\", \"cyber-forget-corpus\", \"cyber-retain-corpus\"] # Everything on cais/wmdp-corpora\n",
    "for name in dataset_names:\n",
    "    if not os.path.exists(f\"wmdp_data/{name}.jsonl\"):\n",
    "        dataset = load_dataset('cais/wmdp-corpora', name)\n",
    "        dataset[\"train\"].to_json(f'wmdp_data/{name}.jsonl', batch_size=128)\n",
    "\n",
    "\"\"\"TODO: Replace with bio loading code here from local file\"\"\"\n",
    "# save_path = f\"wmdp_data/bio-forget-corpus.jsonl\"\n",
    "# if not os.path.exists(save_path):\n",
    "#     dataset = load_dataset(\"CindyXWu/wmdp-private\", token=hf_api_key)\n",
    "#     dataset[\"train\"].to_json(save_path, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(file_path: str) -> dict:\n",
    "    with open(file_path, 'r') as stream:\n",
    "        config = yaml.safe_load(stream)\n",
    "    config = OmegaConf.create(config)\n",
    "    return config\n",
    "\n",
    "config: OmegaConf = load_config(\"wmdp_rmu_lat_config.yaml\")\n",
    "args = config.rmu_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_attack(\n",
    "    model,\n",
    "    pgd_config,\n",
    "    batch: dict[str, torch.Tensor], \n",
    "    do_grad_step: bool,\n",
    "    pca_kwargs: Optional[dict] = None,\n",
    ") -> None:\n",
    "    config = pgd_config\n",
    "    return projected_gradient_descent(\n",
    "        batch=batch,\n",
    "        model=model,\n",
    "        model_layers_module=\"model.layers\",\n",
    "        layer_list=config.pgd_layers,\n",
    "        epsilon=config.epsilon,\n",
    "        learning_rate=config.inner_learning_rate,\n",
    "        pgd_iterations=config.pgd_iterations_per_step,\n",
    "        loss_coefs=config.adv_loss_coefs,\n",
    "        log_loss=do_grad_step,\n",
    "        device=model.device,\n",
    "        pca_kwargs=pca_kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models(config):\n",
    "    updated_model, tokenizer = load_model(hf_api_key)\n",
    "    frozen_model, _ = load_model(hf_api_key)\n",
    "    updated_model = updated_model.train()\n",
    "    return updated_model, tokenizer, frozen_model\n",
    "\n",
    "\n",
    "def create_control_vectors(forget_data_list, updated_model, steering_coef):\n",
    "    \"\"\"Make an RMU control vector separately for each dataset (forget_data_list contains both bio and cyber).\"\"\"\n",
    "    control_vectors_list = []\n",
    "    for _ in range(len(forget_data_list)):\n",
    "        random_vector = torch.rand(1, 1, updated_model.config.hidden_size, dtype=updated_model.dtype, device=updated_model.device)\n",
    "        control_vec = random_vector / torch.norm(random_vector) * steering_coef\n",
    "        control_vectors_list.append(control_vec)\n",
    "    return control_vectors_list\n",
    "\n",
    "\n",
    "def prepare_inputs(tokenizer, unlearn_batch, retain_batch, topic_idx, device):\n",
    "    max_length = 512 if topic_idx == 0 else 768\n",
    "    unlearn_inputs = tokenizer(unlearn_batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
    "    retain_inputs = tokenizer(retain_batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "    return unlearn_inputs, retain_inputs\n",
    "\n",
    "\n",
    "def prepare_pgd_batch(unlearn_inputs, retain_inputs, updated_model, tokenizer, retain_batch, unlearn_batch):\n",
    "    adv_labels_mask = torch.zeros_like(unlearn_inputs[\"input_ids\"], dtype=bool)\n",
    "    def_labels_mask = torch.zeros_like(retain_inputs[\"input_ids\"], dtype=bool)\n",
    "    for b, example in enumerate(retain_batch):\n",
    "        def_labels_mask[b, :len(tokenizer(example)[\"input_ids\"])] = True\n",
    "    for b, example in enumerate(unlearn_batch):\n",
    "        adv_labels_mask[b, :len(tokenizer(example)[\"input_ids\"])] = True\n",
    "\n",
    "    return {\n",
    "        \"def_tokens\": retain_inputs[\"input_ids\"].to(updated_model.device),\n",
    "        \"adv_tokens\": unlearn_inputs[\"input_ids\"].to(updated_model.device),\n",
    "        \"adv_labels_mask\": adv_labels_mask.to(updated_model.device),\n",
    "        \"def_labels_mask\": def_labels_mask.to(updated_model.device)\n",
    "    }\n",
    "\n",
    "\n",
    "def disable_model_gradients(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "\n",
    "def enable_model_gradients(model):\n",
    "    for i in range(model.config.num_hidden_layers):\n",
    "        model.get_submodule(\"model.layers\")[i].requires_grad_(True)\n",
    "\n",
    "\n",
    "def compute_forget_loss(updated_model, unlearn_inputs, control_vec, module):\n",
    "    activations = forward_with_cache(updated_model, unlearn_inputs, module=module, no_grad=False).to(updated_model.device)\n",
    "    return torch.nn.functional.mse_loss(activations, control_vec)\n",
    "\n",
    "\n",
    "def compute_retain_loss(updated_model, retain_inputs, frozen_model, alpha, updated_module, frozen_module):\n",
    "    updated_retain_activations = forward_with_cache(updated_model, retain_inputs, module=updated_module, no_grad=False).to(updated_model.device)\n",
    "    frozen_retain_activations = forward_with_cache(frozen_model, retain_inputs, module=frozen_module, no_grad=True).to(updated_model.device)\n",
    "    return torch.nn.functional.mse_loss(updated_retain_activations, frozen_retain_activations) * alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function for computing RMU loss comes from our codebase with some formalization to match the form of other loss functions, but a more simplified version can be found in the helper functions above for each of the two functions (our codebase will have 'RMU' explicit in the function name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latent_at.lat_helpers import compute_rmu_retain_loss, compute_rmu_forget_loss\n",
    "\n",
    "\n",
    "def run_rmu(\n",
    "    forget_data_list,\n",
    "    retain_data_list,\n",
    "    config,\n",
    "):\n",
    "    \"\"\"At the moment, support for adversary attacks as steering vectors is limited to the first adversary in the returned list.\n",
    "    This is the first one specified in the config list, so make sure it's the right one for the right layer!\n",
    "    \"\"\"\n",
    "    args = config.rmu_config\n",
    "    run_name = f\"rmutrainablelayers-{args.layer_ids}_rmulayer-{args.layer_id}_pgdlayers-{config.pgd_config.pgd_layers}_eps-{config.pgd_config.epsilon}_defsteps-{config.def_config.model_iterations_per_step}_pdsteps-{config.pgd_config.pgd_iterations_per_step}_alpha-{args.alpha}_{datetime.datetime.now().strftime('%d-%m_%H-%M-%S')}\"\n",
    "    logger_params = {\n",
    "        \"name\": run_name,\n",
    "        \"project\": \"rmu_lat\",\n",
    "        \"settings\": wandb.Settings(start_method=\"thread\"),\n",
    "        \"config\": OmegaConf.to_container(config),\n",
    "        \"mode\": \"online\"\n",
    "    }\n",
    "    wandb.init(**logger_params, entity=\"quirky_lats_at_mats\")\n",
    "    print(\"====rmu Config====\")\n",
    "    print(\"\\n\".join(f\"{k}={v}\" for k,v in config.rmu_config.items()))\n",
    "    print(\"=====\")\n",
    "\n",
    "    updated_model, tokenizer, frozen_model = initialize_models(config)\n",
    "\n",
    "    params = get_params(updated_model, args.layer_ids, args.param_ids)\n",
    "    optimizer = AdamW(params, lr=args.lr)\n",
    "\n",
    "    frozen_module = eval(args.module_str.format(model_name=\"frozen_model\", layer_id=args.layer_id))\n",
    "    updated_module = eval(args.module_str.format(model_name=\"updated_model\", layer_id=args.layer_id))\n",
    "\n",
    "    control_vectors_list = create_control_vectors(forget_data_list, updated_model, args.steering_coef)\n",
    "\n",
    "    num_batches = args.max_num_batches\n",
    "    truncation_side = tokenizer.truncation_side\n",
    "    tokenizer.truncation_side=\"right\"\n",
    "    save_path = f\"wmdp_rmulat_models/{run_name}\"\n",
    "\n",
    "    for epoch in range(1):\n",
    "        print(f\"======= Epoch {epoch} =======\")\n",
    "        with tqdm.tqdm(total=num_batches) as pbar:\n",
    "            for idx in range(num_batches):\n",
    "                clear_hooks(updated_model)\n",
    "\n",
    "                if idx % config.eval_steps == 0:\n",
    "                    updated_model.save_pretrained(save_path)\n",
    "                    tokenizer.save_pretrained(save_path)\n",
    "                    with torch.inference_mode():\n",
    "                        eval_and_log(config, updated_model, idx)\n",
    "\n",
    "                topic_idx, batch_idx = idx % len(forget_data_list), idx // len(forget_data_list)\n",
    "                control_vec = control_vectors_list[topic_idx]\n",
    "                unlearn_batch = forget_data_list[topic_idx][batch_idx]\n",
    "                retain_batch = retain_data_list[topic_idx][batch_idx]\n",
    "\n",
    "                unlearn_inputs, retain_inputs = prepare_inputs(tokenizer, unlearn_batch, retain_batch, topic_idx, updated_model.device)\n",
    "\n",
    "                if args.use_pgd:\n",
    "                    pgd_batch = prepare_pgd_batch(unlearn_inputs, retain_inputs, updated_model, tokenizer, retain_batch, unlearn_batch)\n",
    "                    disable_model_gradients()\n",
    "                    losses, hooks, _ = train_attack(\n",
    "                        updated_model,\n",
    "                        config.pgd_config,\n",
    "                        batch=pgd_batch,\n",
    "                        do_grad_step=True,\n",
    "                    )\n",
    "                    log(losses, idx=idx)\n",
    "                    enable_model_gradients(updated_model)\n",
    "                    \n",
    "                    for hook in hooks:\n",
    "                        hook.enabled = False\n",
    "                    for _ in range(config.def_config.model_iterations_per_step):\n",
    "                        # Keep PGD perturbations on for forget loss only\n",
    "                        for hook in hooks:\n",
    "                            hook.enabled = True\n",
    "                        unlearn_loss = compute_rmu_forget_loss(updated_model, unlearn_inputs, control_vec, updated_module)\n",
    "\n",
    "                        for hook in hooks:\n",
    "                            hook.enabled = False\n",
    "                        retain_loss = compute_rmu_retain_loss(updated_model, retain_inputs, frozen_model, args.alpha, updated_module, frozen_module)\n",
    "\n",
    "                        loss = unlearn_loss + retain_loss\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                else: # Just code from CUT\n",
    "                    unlearn_loss = compute_rmu_forget_loss(updated_model, unlearn_inputs, control_vec, updated_module)\n",
    "                    # Can also use compute_forget_loss()\n",
    "\n",
    "                    retain_loss = compute_rmu_retain_loss(updated_model, retain_inputs, frozen_model, args.alpha, updated_module, frozen_module)\n",
    "                    # Can also use compute_retain_loss()\n",
    "\n",
    "                    loss = unlearn_loss + retain_loss\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                loss_dict = {\"total_loss\": loss.item(), \"unlearn_loss\": unlearn_loss.item(), \"retain_loss\": retain_loss.item()}\n",
    "                log(loss_dict, idx=idx)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "    tokenizer.truncation_side = truncation_side\n",
    "    # Save model\n",
    "    path = f\"wmdp_rmulat_models/{run_name}\"\n",
    "    clear_hooks(updated_model)\n",
    "    updated_model.save_pretrained(path)\n",
    "    tokenizer.save_pretrained(path)\n",
    "    print(f\"Saved model to {path}\")\n",
    "    print(\"Evaluating saved model with harness\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forget_data_list, retain_data_list = get_data(\n",
    "    args.forget_corpora,\n",
    "    args.retain_corpora,\n",
    "    args.min_len,\n",
    "    args.max_len,\n",
    "    args.batch_size,\n",
    ")\n",
    "\n",
    "run_rmu(\n",
    "    forget_data_list,\n",
    "    retain_data_list,\n",
    "    config,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
